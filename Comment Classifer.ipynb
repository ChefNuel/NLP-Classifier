{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90242227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc9eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ceca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                               comments           Label\n",
       "0    Currently dealing with them, left 13 months in...  Medical Doctor\n",
       "1    Do you mind sharing how many years out you are...  Medical Doctor\n",
       "2    I'm a physician, my spouse is a vet so I have ...  Medical Doctor\n",
       "3    My thoughts exactly - human physician here. Ev...  Medical Doctor\n",
       "4    Quit. Leave. Don’t show up. They will figure o...  Medical Doctor\n",
       "..                                                 ...             ...\n",
       "117  What's the solution to Mysterium VPN wrecking ...           Other\n",
       "118  Cool but What about the original decentralised...           Other\n",
       "119  Fellow european here. I would guess it greatly...           Other\n",
       "120  Anyone who's made it to 4th year can be a vete...           Other\n",
       "121  i ended up calling the vet clinic that partner...           Other\n",
       "\n",
       "[122 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac878d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\femibewaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from text\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove mutiple spaces with single space\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()  \n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa37782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the comments\n",
    "df['comments'] = df['comments'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b21185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in the dataset after dropping:  0\n",
      "Number of NaNs in y_train:  0\n",
      "Number of NaNs in y_test:  0\n",
      "Number of NaNs in y_train after filling:  0\n",
      "Number of NaNs in y_test after filling:  0\n",
      "Unexpected labels in y_train: Series([], Name: Label, dtype: int64)\n",
      "Unexpected labels in y_test: Series([], Name: Label, dtype: int64)\n",
      "Number of NaNs in y_train after mapping:  0\n",
      "Number of NaNs in y_test after mapping:  0\n"
     ]
    }
   ],
   "source": [
    "# Check for and drop rows with missing target values and comments\n",
    "df = df.dropna(subset=['comments', 'Label'])\n",
    "\n",
    "# Verify no NaNs in the dataset\n",
    "print(\"Number of NaNs in the dataset after dropping: \", df.isnull().sum().sum())\n",
    "\n",
    "# Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['comments'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify no NaNs in the target variable after split\n",
    "print(\"Number of NaNs in y_train: \", y_train.isnull().sum())\n",
    "print(\"Number of NaNs in y_test: \", y_test.isnull().sum())\n",
    "\n",
    "# Check for NaN values in y_train and y_test and fill them with 'other'\n",
    "y_train = y_train.fillna(\"other\")\n",
    "y_test = y_test.fillna(\"other\")\n",
    "\n",
    "# Ensure there are no NaNs in the labels\n",
    "print(\"Number of NaNs in y_train after filling: \", y_train.isnull().sum())\n",
    "print(\"Number of NaNs in y_test after filling: \", y_test.isnull().sum())\n",
    "\n",
    "# Convert labels to integers\n",
    "label_map = {'Medical Doctor': 0, 'Veterinarian': 1, 'Other': 2}\n",
    "y_train = y_train.map(label_map)\n",
    "y_test = y_test.map(label_map)\n",
    "\n",
    "# Identify unexpected labels\n",
    "unexpected_labels_train = y_train[y_train.isna()]\n",
    "unexpected_labels_test = y_test[y_test.isna()]\n",
    "\n",
    "print(\"Unexpected labels in y_train:\", unexpected_labels_train)\n",
    "print(\"Unexpected labels in y_test:\", unexpected_labels_test)\n",
    "\n",
    "# Fill NaNs resulting from unexpected labels with the value for 'other'\n",
    "y_train = y_train.fillna(label_map['Other'])\n",
    "y_test = y_test.fillna(label_map['Other'])\n",
    "\n",
    "# Verify no NaNs in the mapped labels\n",
    "print(\"Number of NaNs in y_train after mapping: \", y_train.isnull().sum())\n",
    "print(\"Number of NaNs in y_test after mapping: \", y_test.isnull().sum())\n",
    "\n",
    "# Convert to integer arrays\n",
    "y_train = y_train.astype(int).values\n",
    "y_test = y_test.astype(int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b8e690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7589458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance in training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_tfidf_res, y_train_res = smote.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4be888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train before fitting logistic regression:\n",
      "Class 0: 26\n",
      "Class 1: 36\n",
      "Class 2: 35\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', max_iter=200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=200, class_weight='balanced',penalty='l2')\n",
    "\n",
    "# Check class distribution again before fitting\n",
    "unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution in y_train before fitting logistic regression:\")\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "\n",
    "    \n",
    "log_reg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7bf319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model 2: BERT\n",
    "# Initialize BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4667dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def encode_texts(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d011925",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = encode_texts(X_train, bert_tokenizer)\n",
    "X_test_enc = encode_texts(X_test, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the BERT model\n",
    "bert_model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-5),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d207f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 171s 21s/step - loss: 1.1375 - sparse_categorical_accuracy: 0.3448 - val_loss: 1.1130 - val_sparse_categorical_accuracy: 0.3000\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 115s 19s/step - loss: 1.0667 - sparse_categorical_accuracy: 0.3793 - val_loss: 1.0998 - val_sparse_categorical_accuracy: 0.3000\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 114s 19s/step - loss: 1.0549 - sparse_categorical_accuracy: 0.4368 - val_loss: 1.0901 - val_sparse_categorical_accuracy: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x153491754d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.fit(\n",
    "    X_train_enc.data,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "546a388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify new text input\n",
    "def classify_text(comment, log_reg_model, bert_model, tfidf_vectorizer, bert_tokenizer):\n",
    "    # Preprocess and vectorize the comment for Logistic Regression\n",
    "    comment_processed = preprocess_text(comment)\n",
    "    comment_tfidf = tfidf_vectorizer.transform([comment_processed])\n",
    "    \n",
    "    # Predict using Logistic Regression\n",
    "    pred_lr = log_reg_model.predict(comment_tfidf)\n",
    "    \n",
    "    # Tokenize for BERT prediction\n",
    "    comment_tokenized = bert_tokenizer(comment_processed, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
    "    pred_bert_logits = bert_model.predict(comment_tokenized)[0]\n",
    "    pred_bert = np.argmax(pred_bert_logits, axis=1)\n",
    "    \n",
    "    # Map integer labels back to categories\n",
    "    inv_label_map = {0: 'medical doctor', 1: 'veterinarian', 2: 'other'}\n",
    "    \n",
    "    return {\n",
    "        \"Logistic Regression Prediction\": inv_label_map[pred_lr[0]],\n",
    "        \"BERT Prediction\": inv_label_map[pred_bert[0]]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cce59f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "{'Logistic Regression Prediction': 'veterinarian', 'BERT Prediction': 'veterinarian'}\n"
     ]
    }
   ],
   "source": [
    "# Test the function with a new comment\n",
    "new_comment = \"It's was very hard for me too at the beginning, I was stressing out, but with time and training it will be easier ! I was doing 1h30 at the beginning and now I m doing 30 min! Just have to be patient and train :)|Hello, thank you for this encouragement and sharing your experience! Im actually working in small GP, but I will look for a new place with someone who will be able to train me instead to tell me I'm not good enough. Thank you :)\"\n",
    "print(classify_text(new_comment, log_reg, bert_model, vectorizer, bert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16727b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
